{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37a62efa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'aiohttp._websocket.reader'; 'aiohttp._websocket' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbs4\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hub\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocument_loaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WebBaseLoader\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocuments\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_text_splitters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\ai\\Lib\\site-packages\\langchain_community\\document_loaders\\__init__.py:740\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m    738\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m) -> Any:\n\u001b[32m    739\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _module_lookup:\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m         module = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_module_lookup\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    741\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m    742\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\ai\\Lib\\importlib\\__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     88\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\ai\\Lib\\site-packages\\langchain_community\\document_loaders\\web_base.py:8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, AsyncIterator, Dict, Iterator, List, Optional, Sequence, Union\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01maiohttp\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deprecated\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\ai\\Lib\\site-packages\\aiohttp\\__init__.py:6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Tuple\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hdrs \u001b[38;5;28;01mas\u001b[39;00m hdrs\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclient\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      7\u001b[39m     BaseConnector,\n\u001b[32m      8\u001b[39m     ClientConnectionError,\n\u001b[32m      9\u001b[39m     ClientConnectionResetError,\n\u001b[32m     10\u001b[39m     ClientConnectorCertificateError,\n\u001b[32m     11\u001b[39m     ClientConnectorDNSError,\n\u001b[32m     12\u001b[39m     ClientConnectorError,\n\u001b[32m     13\u001b[39m     ClientConnectorSSLError,\n\u001b[32m     14\u001b[39m     ClientError,\n\u001b[32m     15\u001b[39m     ClientHttpProxyError,\n\u001b[32m     16\u001b[39m     ClientOSError,\n\u001b[32m     17\u001b[39m     ClientPayloadError,\n\u001b[32m     18\u001b[39m     ClientProxyConnectionError,\n\u001b[32m     19\u001b[39m     ClientRequest,\n\u001b[32m     20\u001b[39m     ClientResponse,\n\u001b[32m     21\u001b[39m     ClientResponseError,\n\u001b[32m     22\u001b[39m     ClientSession,\n\u001b[32m     23\u001b[39m     ClientSSLError,\n\u001b[32m     24\u001b[39m     ClientTimeout,\n\u001b[32m     25\u001b[39m     ClientWebSocketResponse,\n\u001b[32m     26\u001b[39m     ClientWSTimeout,\n\u001b[32m     27\u001b[39m     ConnectionTimeoutError,\n\u001b[32m     28\u001b[39m     ContentTypeError,\n\u001b[32m     29\u001b[39m     Fingerprint,\n\u001b[32m     30\u001b[39m     InvalidURL,\n\u001b[32m     31\u001b[39m     InvalidUrlClientError,\n\u001b[32m     32\u001b[39m     InvalidUrlRedirectClientError,\n\u001b[32m     33\u001b[39m     NamedPipeConnector,\n\u001b[32m     34\u001b[39m     NonHttpUrlClientError,\n\u001b[32m     35\u001b[39m     NonHttpUrlRedirectClientError,\n\u001b[32m     36\u001b[39m     RedirectClientError,\n\u001b[32m     37\u001b[39m     RequestInfo,\n\u001b[32m     38\u001b[39m     ServerConnectionError,\n\u001b[32m     39\u001b[39m     ServerDisconnectedError,\n\u001b[32m     40\u001b[39m     ServerFingerprintMismatch,\n\u001b[32m     41\u001b[39m     ServerTimeoutError,\n\u001b[32m     42\u001b[39m     SocketTimeoutError,\n\u001b[32m     43\u001b[39m     TCPConnector,\n\u001b[32m     44\u001b[39m     TooManyRedirects,\n\u001b[32m     45\u001b[39m     UnixConnector,\n\u001b[32m     46\u001b[39m     WSMessageTypeError,\n\u001b[32m     47\u001b[39m     WSServerHandshakeError,\n\u001b[32m     48\u001b[39m     request,\n\u001b[32m     49\u001b[39m )\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclient_middleware_digest_auth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DigestAuthMiddleware\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclient_middlewares\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ClientHandlerType, ClientMiddlewareType\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\ai\\Lib\\site-packages\\aiohttp\\client.py:41\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01myarl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m URL\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hdrs, http, payload\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_websocket\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WebSocketDataQueue\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mabc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AbstractCookieJar\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclient_exceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     44\u001b[39m     ClientConnectionError,\n\u001b[32m     45\u001b[39m     ClientConnectionResetError,\n\u001b[32m   (...)\u001b[39m\u001b[32m     72\u001b[39m     WSServerHandshakeError,\n\u001b[32m     73\u001b[39m )\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'aiohttp._websocket.reader'; 'aiohttp._websocket' is not a package"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import getpass\n",
    "# import os\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from typing import TypedDict, List, Dict, Any\n",
    "import hashlib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d650b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置API信息（替换为你的第三方平台信息）\n",
    "API_BASE_URL = \"https://api2.aigcbest.top/v1\"  # 第三方API基础地址\n",
    "API_KEY = \"sk-5hmjTgNvEwpzxliLy8Ub6SBkjdt5GkotJUcr9Y8HoW8CQ7bX\"  # 你的第三方平台API Key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c333a34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Decomposition is a process used in planning within an LLM-powered autonomous agent system. It involves breaking down a complicated task, which usually consists of many steps, into smaller and simpler tasks. This approach helps the agent plan effectively by making large, complex tasks more manageable. One standard technique for task decomposition is Chain of Thought (CoT), which instructs the model to \"think step by step\" to enhance performance on complex tasks by using more computation during testing to decompose these tasks into smaller ones. Additionally, Tree of Thoughts extends CoT by exploring multiple reasoning paths at each step, forming a tree structure, and utilizing search strategies like BFS or DFS to evaluate each state's potential solutions. Task decomposition can be achieved through simple prompting, task-specific instructions, or human input.\n"
     ]
    }
   ],
   "source": [
    "# 1. 加载并分割文档（这部分无需依赖模型，保持不变）\n",
    "class WebBaseLoader:\n",
    "    def __init__(self, web_paths, bs_kwargs):\n",
    "        self.web_paths = web_paths\n",
    "        self.bs_kwargs = bs_kwargs\n",
    "\n",
    "    def load(self):\n",
    "        docs = []\n",
    "        for url in self.web_paths:\n",
    "            response = requests.get(url)\n",
    "            strainer = self.bs_kwargs.get('parse_only')\n",
    "            soup = BeautifulSoup(response.text, 'html.parser', parse_only=strainer)\n",
    "            content = soup.get_text()\n",
    "            docs.append({\"page_content\": content, \"metadata\": {\"source\": url}})\n",
    "        return docs\n",
    "\n",
    "# 文档分割器\n",
    "class RecursiveCharacterTextSplitter:\n",
    "    def __init__(self, chunk_size, chunk_overlap):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "\n",
    "    def split_documents(self, docs):\n",
    "        chunks = []\n",
    "        for doc in docs:\n",
    "            content = doc[\"page_content\"]\n",
    "            start = 0\n",
    "            while start < len(content):\n",
    "                end = start + self.chunk_size\n",
    "                chunk = content[start:end]\n",
    "                chunks.append({\n",
    "                    \"page_content\": chunk,\n",
    "                    \"metadata\": doc[\"metadata\"]\n",
    "                })\n",
    "                start = end - self.chunk_overlap\n",
    "        return chunks\n",
    "\n",
    "# 加载并分割文档\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# 2. 用HTTP请求实现向量存储（替代vector_store）\n",
    "class HTTPVectorStore:\n",
    "    def __init__(self, api_base, api_key):\n",
    "        self.api_base = api_base\n",
    "        self.api_key = api_key\n",
    "        self.embeddings = {}  # 本地存储向量（实际应用中可能需要数据库）\n",
    "\n",
    "    def get_embedding(self, text: str) -> List[float]:\n",
    "        \"\"\"通过HTTP请求获取文本嵌入向量\"\"\"\n",
    "        url = f\"{self.api_base}/embeddings\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        data = {\n",
    "            \"model\": \"text-embedding-3-large\",  # 嵌入模型\n",
    "            \"input\": text,\n",
    "            \"encoding_format\": \"float\"\n",
    "        }\n",
    "        response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "        return response.json()[\"data\"][0][\"embedding\"]\n",
    "\n",
    "    def add_documents(self, documents):\n",
    "        \"\"\"将文档分割添加到向量存储\"\"\"\n",
    "        for doc in documents:\n",
    "            # 生成唯一ID（用于后续检索）\n",
    "            doc_id = hashlib.md5(doc[\"page_content\"].encode()).hexdigest()\n",
    "            # 获取嵌入向量\n",
    "            embedding = self.get_embedding(doc[\"page_content\"])\n",
    "            self.embeddings[doc_id] = {\n",
    "                \"embedding\": embedding,\n",
    "                \"document\": doc\n",
    "            }\n",
    "\n",
    "    def similarity_search(self, query: str, top_k: int = 3) -> List[Dict]:\n",
    "        \"\"\"通过HTTP请求实现相似性检索\"\"\"\n",
    "        # 获取查询的嵌入向量\n",
    "        query_embedding = self.get_embedding(query)\n",
    "        \n",
    "        # 简单的余弦相似度计算（实际应用可能在服务端完成）\n",
    "        results = []\n",
    "        for doc_id, item in self.embeddings.items():\n",
    "            doc_embedding = item[\"embedding\"]\n",
    "            # 计算余弦相似度\n",
    "            similarity = self._cosine_similarity(query_embedding, doc_embedding)\n",
    "            results.append({\n",
    "                \"similarity\": similarity,\n",
    "                \"document\": item[\"document\"]\n",
    "            })\n",
    "        \n",
    "        # 按相似度排序并返回前top_k个结果\n",
    "        results.sort(key=lambda x: x[\"similarity\"], reverse=True)\n",
    "        return [item[\"document\"] for item in results[:top_k]]\n",
    "\n",
    "    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:\n",
    "        \"\"\"计算两个向量的余弦相似度\"\"\"\n",
    "        dot_product = sum(a * b for a, b in zip(vec1, vec2))\n",
    "        norm1 = sum(a **2 for a in vec1)** 0.5\n",
    "        norm2 = sum(b **2 for b in vec2)** 0.5\n",
    "        return dot_product / (norm1 * norm2) if norm1 and norm2 else 0\n",
    "\n",
    "# 初始化向量存储并添加文档\n",
    "vector_store = HTTPVectorStore(API_BASE_URL, API_KEY)\n",
    "vector_store.add_documents(all_splits)\n",
    "\n",
    "# 3. 用HTTP请求实现大模型调用（替代llm.invoke）\n",
    "def call_chat_model(messages: List[Dict[str, str]], model: str = \"gpt-4o\") -> str:\n",
    "    \"\"\"通过HTTP请求调用聊天模型\"\"\"\n",
    "    url = f\"{API_BASE_URL}/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "# 4. 定义RAG流程（状态和步骤）\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Dict]\n",
    "    answer: str\n",
    "\n",
    "def retrieve(state: State) -> Dict[str, List[Dict]]:\n",
    "    \"\"\"检索相关文档\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "def generate(state: State) -> Dict[str, str]:\n",
    "    \"\"\"生成回答\"\"\"\n",
    "    # 构建提示词（替代hub.pull的prompt）\n",
    "    context = \"\\n\\n\".join(doc[\"page_content\"] for doc in state[\"context\"])\n",
    "    prompt = f\"\"\"\n",
    "    基于以下上下文回答问题，只使用提供的信息，不要编造内容。\n",
    "    上下文: {context}\n",
    "    问题: {state[\"question\"]}\n",
    "    回答:\n",
    "    \"\"\"\n",
    "    \n",
    "    # 调用聊天模型\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    answer = call_chat_model(messages)\n",
    "    return {\"answer\": answer}\n",
    "\n",
    "# 5. 执行RAG流程\n",
    "def run_rag(question: str) -> str:\n",
    "    state = {\"question\": question, \"context\": [], \"answer\": \"\"}\n",
    "    state.update(retrieve(state))\n",
    "    state.update(generate(state))\n",
    "    return state[\"answer\"]\n",
    "# 测试\n",
    "if __name__ == \"__main__\":\n",
    "    question = \"What is Task Decomposition?\"\n",
    "    print(run_rag(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5be3a03",
   "metadata": {},
   "source": [
    "1 用数据让rag根据数据进行回答\n",
    "2 尝试构建本地模型库，让大模型根据数据选择模型并给出解释：\n",
    "    库中预先存放工业故障诊断常用的机器学习 / 深度学习模型文件（如 LSTM、TCN、KNN、SVM、Transformer 等，涵盖 FD 1.0/1.5 阶段的经典模型及 FD 2.0 的 Time-GPT 等），每个模型对应独立的可执行代码文件（如.py脚本）和调用接口（如函数入口、参数说明）。\n",
    "    配套数据库：存储模型的 “元信息”，包括：\n",
    "    模型基本属性：适用场景（如 “长时预测”“非线性故障检测”“小数据集任务”）、输入数据要求（如时间序列维度、数据格式）、性能指标（如在历史数据上的 Precision/Recall/F1）；\n",
    "    模型关联信息：调用路径、依赖库（如 Pandas、Scikit-learn、PyTorch）、更新时间（用于判断是否需维护）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b736310e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
