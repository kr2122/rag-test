from operator import itemgetter
from pathlib import Path

from langchain_community.document_loaders import DirectoryLoader,TextLoader
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda, RunnableParallel, RunnablePassthrough
from langchain_openai import ChatOpenAI
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_ollama import ChatOllama
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
# from langchain_community.embeddings import OllamaEmbeddings
from langchain_ollama.embeddings import OllamaEmbeddings
from data_analysis import analyze_excel
from model_py import train_model

# 1. è®¾ç½®æ¨¡å‹

# llm = ChatOpenAI(
#     model="deepseek-ai/DeepSeek-R1",
#     temperature=0,
#     api_key="sk-5hmjTgNvEwpzxliLy8Ub6SBkjdt5GkotJUcr9Y8HoW8CQ7bX",
#     base_url="https://api2.aigcbest.top/v1"
# )
llm = ChatOllama(
    model="qwen2.5:7b",
    # temperature=0,
    # api_key="sk-5hmjTgNvEwpzxliLy8Ub6SBkjdt5GkotJUcr9Y8HoW8CQ7bX",
    base_url="http://10.108.13.254:11434"
)

# nomic-embed-text:latest
# embed_model = HuggingFaceEmbeddings(
#     model_name="BAAI/bge-base-zh-v1.5"
# )
embed_model = OllamaEmbeddings(
    model="embeddinggemma:300m",
    # temperature=0,
    # api_key="sk-5hmjTgNvEwpzxliLy8Ub6SBkjdt5GkotJUcr9Y8HoW8CQ7bX",
    base_url="http://10.108.13.254:11434"
)



# 2. æ•°æ®å¤„ç†
file_dir = Path('./lang_rag/doc')
# print(file_dir)
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=100
)
vector_store = Chroma(embedding_function=embed_model,persist_directory="./vector_db")
retriever = vector_store.as_retriever(search_kwargs={"k": 5})

prompt_template = PromptTemplate.from_template(
    """
    ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„RAGåŠ©æ‰‹
    åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼Œç»“åˆæä¾›çš„ä¿¡æ¯ç»™å‡ºé€‚åˆçš„æ¨¡å‹ï¼Œç›´æ¥è¯´å‡ºæ¨¡å‹åç§°,ä¸ç”¨è¾“å‡ºå¤šä½™å†…å®¹ã€‚
    ä¾‹å¦‚ï¼š
    qï¼šæ ¹æ®ä»¥ä¸‹æ•°æ®ç‰¹å¾ï¼Œæ¨èæœ€åˆé€‚çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼š
    aï¼šxgboost
    ä¸Šä¸‹æ–‡: {context}
    é—®é¢˜: {question}
    å›ç­”:
    """
)



# 3. ç¼–é€ é“¾

# chain = {
#     RunnableParallel({"context": retriever, "question": RunnablePassthrough()})
#     | prompt_template
#     | llm
#     | StrOutputParser()
# }
chain = (
    RunnableParallel({
        "context": retriever,
        "question": RunnablePassthrough()
    })
    | prompt_template  # å°†å¹¶è¡Œç»“æœä¼ å…¥æç¤ºè¯æ¨¡æ¿
    | llm  # ä¼ å…¥å¤§æ¨¡å‹
    | StrOutputParser()  # è§£æè¾“å‡ºä¸ºå­—ç¬¦ä¸²
)



# 4.åˆå§‹åŒ–çŸ¥è¯†åº“
if __name__ == "__main__":
    # docs = DirectoryLoader(str(file_dir),loader_cls=TextLoader,loader_kwargs={"encoding": "utf-8"}).load()
    # all_splits = text_splitter.split_documents(docs)
    # vector_store.add_documents(all_splits)
#æ›´æ¢åµŒå…¥æ¨¡å‹è¦åˆ é™¤æ•°æ®åº“ï¼Œé‡æ–°åˆå§‹åŒ–
    data_path = "./lang_rag/data/environment_data_export_2025-10-16_164127.csv"  # ä½ è‡ªå·±çš„Excelæ–‡ä»¶è·¯å¾„
    data_summary = analyze_excel(data_path)
    print("ğŸ“Š æ•°æ®åˆ†æç»“æœï¼š")
    print(data_summary)
    print("â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”")

    # 3. å°†æ•°æ®æè¿°ä½œä¸ºé—®é¢˜è¾“å…¥RAGé“¾
    question = f"æ ¹æ®ä»¥ä¸‹æ•°æ®ç‰¹å¾ï¼Œæ¨èæœ€åˆé€‚çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼š{data_summary}"
    result = chain.invoke(question)
    print("ğŸ¤– æ¨¡å‹æ¨èç»“æœï¼š")
    print(result)

    df = pd.read_csv(data_path)
    
    try:
        model, y_pred = train_model(result, df)
        print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    except Exception as e:
        print(f"âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")







    # # ç®€å•å¤„ç†ï¼šå‡è®¾æœ€åä¸€åˆ—ä¸ºç›®æ ‡åˆ—
    # X = df.iloc[:, :-1].values
    # y = df.iloc[:, -1].values

    # # è‹¥æ˜¯æ—¶é—´åºåˆ—ï¼Œå¯æ”¹ä¸ºçª—å£åŒ–è¾“å…¥ï¼Œæš‚æ—¶ç”¨æ™®é€šæ‹†åˆ†
    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # # å¦‚æœæ¨¡å‹æ˜¯ LSTMï¼Œéœ€è¦ 3D è¾“å…¥ [samples, timesteps, features]
    # if "lstm" in result.lower():
    #     X_train = np.expand_dims(X_train, axis=1)
    #     X_test = np.expand_dims(X_test, axis=1)

    # try:
    #     model, y_pred = train_model(result, X_train, y_train, X_test, y_test)
    #     print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    # except Exception as e:
    #     print(f"âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
    # # print(chain.invoke("æˆ‘æƒ³åšæ•°æ®é¢„æµ‹,å¸®æˆ‘é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„æ¨¡å‹"))









