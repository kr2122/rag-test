# æ–‡ä»¶è·¯å¾„ï¼šlang_rag/models/rnn_model.py
# import torch
# import torch.nn as nn
# import torch.optim as optim
# import numpy as np
# from sklearn.metrics import mean_squared_error, r2_score

# # ====================== #
# #       æ¨¡å‹å®šä¹‰
# # ====================== #
# class RNNModel(nn.Module):
#     def __init__(self, input_size, hidden_size=64, num_layers=2, output_size=1, dropout=0.2):
#         super(RNNModel, self).__init__()
#         self.rnn = nn.RNN(
#             input_size=input_size,
#             hidden_size=hidden_size,
#             num_layers=num_layers,
#             batch_first=True,
#             dropout=dropout,
#             nonlinearity='tanh'
#         )
#         self.fc = nn.Linear(hidden_size, output_size)

#     def forward(self, x):
#         # x: [batch, seq_len, input_size]
#         out, _ = self.rnn(x)
#         out = self.fc(out[:, -1, :])  # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥è¾“å‡º
#         return out


# # ====================== #
# #       è®­ç»ƒå‡½æ•°
# # ====================== #
# def train_rnn(X_train, y_train, X_test, y_test, input_size, epochs=50, lr=0.001):
#     """
#     è®­ç»ƒ RNN æ¨¡å‹
#     å‚æ•°:
#         X_train, y_train, X_test, y_test: numpy æ•°æ®
#         input_size: ç‰¹å¾æ•°é‡
#         epochs: è®­ç»ƒè½®æ•°
#         lr: å­¦ä¹ ç‡
#     """
#     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

#     # æ•°æ®æ ¼å¼è°ƒæ•´
#     X_train = torch.tensor(X_train, dtype=torch.float32).to(device)
#     y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(device)
#     X_test = torch.tensor(X_test, dtype=torch.float32).to(device)
#     y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1).to(device)

#     # æ„å»ºæ¨¡å‹
#     model = RNNModel(input_size=input_size).to(device)
#     criterion = nn.MSELoss()
#     optimizer = optim.Adam(model.parameters(), lr=lr)

#     # ====================== #
#     #       å¼€å§‹è®­ç»ƒ
#     # ====================== #
#     print(f"ğŸš€ å¼€å§‹è®­ç»ƒ RNN æ¨¡å‹ï¼Œå…± {epochs} è½®")

#     for epoch in range(epochs):
#         model.train()
#         optimizer.zero_grad()
#         outputs = model(X_train)
#         loss = criterion(outputs, y_train)
#         loss.backward()
#         optimizer.step()

#         if (epoch + 1) % 10 == 0:
#             print(f"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.6f}")

#     # ====================== #
#     #       æ¨¡å‹è¯„ä¼°
#     # ====================== #
#     model.eval()
#     with torch.no_grad():
#         y_pred = model(X_test).cpu().numpy().flatten()
#         y_true = y_test.cpu().numpy().flatten()
#         mse = mean_squared_error(y_true, y_pred)
#         r2 = r2_score(y_true, y_pred)

#     print("âœ… RNN è®­ç»ƒå®Œæˆ")
#     print(f"ğŸ“‰ MSE: {mse:.6f}, RÂ²: {r2:.4f}")

#     return model, y_pred


import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score


# ====================== #
#       æ¨¡å‹å®šä¹‰
# ====================== #
class RNNModel(nn.Module):
    def __init__(self, input_size, hidden_size=64, num_layers=2, output_size=1, dropout=0.2):
        super(RNNModel, self).__init__()
        self.rnn = nn.RNN(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout,
            nonlinearity='tanh'
        )
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        out, _ = self.rnn(x)
        out = self.fc(out[:, -1, :])  # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥è¾“å‡º
        return out


# ====================== #
#       è®­ç»ƒå‡½æ•°
# ====================== #
def train(df: pd.DataFrame, target_col: str = None, epochs=50, lr=0.001):
    """
    è®­ç»ƒ RNN æ¨¡å‹ï¼ˆç›´æ¥è¾“å…¥ DataFrameï¼‰

    å‚æ•°:
        df: pandas DataFrameï¼ˆåŒ…å«ç‰¹å¾å’Œç›®æ ‡ï¼‰
        target_col: ç›®æ ‡åˆ—åï¼ˆè‹¥ä¸æŒ‡å®šï¼Œåˆ™é»˜è®¤æœ€åä¸€åˆ—ï¼‰
        epochs: è®­ç»ƒè½®æ•°
        lr: å­¦ä¹ ç‡
    """

    print("ğŸ“Š [RNN] å¼€å§‹æ•°æ®é¢„å¤„ç†...")

    # ç¡®å®šç›®æ ‡åˆ—
    if target_col is None:
        target_col = "air_temperature"
    for col in df.columns:
        if df[col].dtype == 'object':
            try:
                # å°è¯•è½¬æ¢æ—¥æœŸæ—¶é—´åˆ—ä¸ºæ—¶é—´æˆ³
                df[col] = pd.to_datetime(df[col]).astype('int64') // 10**9
            except:
                # å…¶ä»–å­—ç¬¦ä¸²åˆ—ç”¨å‡å€¼å¡«å……ï¼ˆæˆ–æ ¹æ®å®é™…æƒ…å†µå¤„ç†ï¼‰
                df[col] = pd.to_numeric(df[col], errors='coerce')
                df[col].fillna(df[col].mean(), inplace=True)

    X = df.drop(columns=[target_col]).values
    y = df[target_col].values

    # æ•°æ®å½’ä¸€åŒ–ï¼ˆå»ºè®®å¯¹ RNN ä½¿ç”¨ï¼‰
    scaler_X = MinMaxScaler()
    scaler_y = MinMaxScaler()

    X_scaled = scaler_X.fit_transform(X)
    y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()

    # æ‹†åˆ†è®­ç»ƒé›†ä¸æµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)

    # RNN è¾“å…¥è¦æ±‚: [samples, timesteps, features]
    X_train = np.expand_dims(X_train, axis=1)
    X_test = np.expand_dims(X_test, axis=1)

    input_size = X_train.shape[2]
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # è½¬æ¢ä¸º tensor
    X_train = torch.tensor(X_train, dtype=torch.float32).to(device)
    y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(device)
    X_test = torch.tensor(X_test, dtype=torch.float32).to(device)
    y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1).to(device)

    # æ„å»ºæ¨¡å‹
    model = RNNModel(input_size=input_size).to(device)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ RNN æ¨¡å‹ï¼Œå…± {epochs} è½®")

    # ====================== #
    #       å¼€å§‹è®­ç»ƒ
    # ====================== #
    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()
        outputs = model(X_train)
        loss = criterion(outputs, y_train)
        loss.backward()
        optimizer.step()

        if (epoch + 1) % 10 == 0 or epoch == 0:
            print(f"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.6f}")

    # ====================== #
    #       æ¨¡å‹è¯„ä¼°
    # ====================== #
    model.eval()
    with torch.no_grad():
        y_pred = model(X_test).cpu().numpy().flatten()
        y_true = y_test.cpu().numpy().flatten()

        # åå½’ä¸€åŒ–
        y_pred_inv = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()
        y_true_inv = scaler_y.inverse_transform(y_true.reshape(-1, 1)).flatten()

        mse = mean_squared_error(y_true_inv, y_pred_inv)
        r2 = r2_score(y_true_inv, y_pred_inv)

    print("âœ… RNN è®­ç»ƒå®Œæˆ")
    print(f"ğŸ“‰ MSE: {mse:.6f}, RÂ²: {r2:.4f}")

    return  y_pred_inv, {"mse": mse, "r2": r2}
# if __name__ == "__main__":
#     # è¯»å–æ•°æ®ï¼ˆæ›¿æ¢ä¸ºä½ çš„æ•°æ®è·¯å¾„ï¼‰
#     data_path = "./lang_rag/data/environment_data_export_2025-10-16_164127.csv"  # ä½ è‡ªå·±çš„Excelæ–‡ä»¶è·¯å¾„
#     df = pd.read_csv(data_path)
#     print(train(df))

