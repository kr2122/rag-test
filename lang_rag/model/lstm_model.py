# import torch
# import torch.nn as nn
# from torch.utils.data import DataLoader, TensorDataset
# import numpy as np

# class LSTMModel(nn.Module):
#     def __init__(self, input_size, hidden_size=64, num_layers=2, output_size=1, dropout=0.2):
#         super(LSTMModel, self).__init__()
#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
#         self.fc = nn.Linear(hidden_size, output_size)

#     def forward(self, x):
#         out, _ = self.lstm(x)
#         out = self.fc(out[:, -1, :])  # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
#         return out


# def train_lstm(X_train, y_train, X_test, y_test, input_size, epochs=50, lr=0.001, batch_size=32):
#     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

#     # è½¬æ¢ä¸ºTensor
#     X_train = torch.tensor(X_train, dtype=torch.float32)
#     y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)
#     X_test = torch.tensor(X_test, dtype=torch.float32)
#     y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)

#     train_dataset = TensorDataset(X_train, y_train)
#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

#     model = LSTMModel(input_size=input_size).to(device)
#     criterion = nn.MSELoss()
#     optimizer = torch.optim.Adam(model.parameters(), lr=lr)

#     model.train()
#     for epoch in range(epochs):
#         for X_batch, y_batch in train_loader:
#             X_batch, y_batch = X_batch.to(device), y_batch.to(device)
#             optimizer.zero_grad()
#             output = model(X_batch)
#             loss = criterion(output, y_batch)
#             loss.backward()
#             optimizer.step()

#         if (epoch+1) % 10 == 0:
#             print(f"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.6f}")

#     model.eval()
#     with torch.no_grad():
#         y_pred = model(X_test.to(device)).cpu().numpy()
#         mse = np.mean((y_pred - y_test.numpy())**2)
#         print(f"âœ… LSTM Test MSE: {mse:.6f}")

#     return model, y_pred






# # æ–‡ä»¶è·¯å¾„ï¼šlang_rag/models/lstm_model.py
# import numpy as np
# import pandas as pd
# import torch
# import torch.nn as nn
# import torch.optim as optim
# from sklearn.preprocessing import MinMaxScaler
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import mean_squared_error, r2_score

# class LSTMModel(nn.Module):
#     def __init__(self, input_size, hidden_size=64, num_layers=2, output_size=1):
#         super(LSTMModel, self).__init__()
#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
#         self.fc = nn.Linear(hidden_size, output_size)

#     def forward(self, x):
#         out, _ = self.lstm(x)
#         return self.fc(out[:, -1, :])

# def train(df):
#     # 1ï¸âƒ£ æ•°æ®å‡†å¤‡
#     X = df.iloc[:, :-1].values
#     y = df.iloc[:, -1].values.reshape(-1, 1)

#     # å½’ä¸€åŒ–
#     scaler_x, scaler_y = MinMaxScaler(), MinMaxScaler()
#     X = scaler_x.fit_transform(X)
#     y = scaler_y.fit_transform(y)

#     # åˆ’åˆ†è®­ç»ƒæµ‹è¯•é›†
#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#     # è½¬ä¸º3Dè¾“å…¥ [samples, timesteps, features]
#     X_train = np.expand_dims(X_train, axis=1)
#     X_test = np.expand_dims(X_test, axis=1)

#     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

#     model = LSTMModel(input_size=X_train.shape[2]).to(device)
#     criterion = nn.MSELoss()
#     optimizer = optim.Adam(model.parameters(), lr=0.001)

#     X_train = torch.tensor(X_train, dtype=torch.float32).to(device)
#     y_train = torch.tensor(y_train, dtype=torch.float32).to(device)
#     X_test = torch.tensor(X_test, dtype=torch.float32).to(device)
#     y_test = torch.tensor(y_test, dtype=torch.float32).to(device)

#     print("ğŸš€ LSTM å¼€å§‹è®­ç»ƒ")
#     for epoch in range(50):
#         model.train()
#         optimizer.zero_grad()
#         outputs = model(X_train)
#         loss = criterion(outputs, y_train)
#         loss.backward()
#         optimizer.step()

#         if (epoch + 1) % 10 == 0:
#             print(f"Epoch [{epoch+1}/50], Loss: {loss.item():.6f}")

#     # æµ‹è¯•é›†é¢„æµ‹
#     model.eval()
#     with torch.no_grad():
#         y_pred = model(X_test).cpu().numpy()
#         y_pred = scaler_y.inverse_transform(y_pred)
#         y_true = scaler_y.inverse_transform(y_test.cpu().numpy())

#     mse = mean_squared_error(y_true, y_pred)
#     r2 = r2_score(y_true, y_pred)
#     print(f"âœ… LSTM å®Œæˆ | MSE={mse:.6f}, RÂ²={r2:.4f}")
#     return model, y_pred
# if __name__ == "__main__":
#     # ç¤ºä¾‹æ•°æ®
#     df= pd.read_csv("./lang_rag/data/environment_data_export_2025-10-16_164127.csv")
#     print(train(df))
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size=64, num_layers=2, output_size=1, dropout=0.2):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(
            input_size, 
            hidden_size, 
            num_layers, 
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0  # ä»…å¤šå±‚æ—¶ä½¿ç”¨dropout
        )
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        out, _ = self.lstm(x)
        return self.fc(out[:, -1, :])  # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥è¾“å‡º


def create_sequences(X, y, seq_len=5):
    """å°†æ•°æ®è½¬æ¢ä¸ºåºåˆ—æ ¼å¼ [samples, timesteps, features]"""
    X_seq, y_seq = [], []
    for i in range(len(X) - seq_len):
        X_seq.append(X[i:i+seq_len])  # è¿ç»­seq_lenä¸ªæ—¶é—´æ­¥ä½œä¸ºè¾“å…¥
        y_seq.append(y[i+seq_len])    # ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥ä½œä¸ºç›®æ ‡
    return np.array(X_seq), np.array(y_seq)


def train(df, seq_len=5, epochs=50, hidden_size=64):
    # 1ï¸âƒ£ æ•°æ®é¢„å¤„ç†
    # æ£€æŸ¥å¹¶å¤„ç†éæ•°å€¼åˆ—ï¼ˆå¦‚æ—¥æœŸæ—¶é—´ï¼‰
    for col in df.columns:
        if df[col].dtype == 'object':
            try:
                # å°è¯•è½¬æ¢æ—¥æœŸæ—¶é—´åˆ—ä¸ºæ—¶é—´æˆ³
                df[col] = pd.to_datetime(df[col]).astype('int64') // 10**9
            except:
                # å…¶ä»–å­—ç¬¦ä¸²åˆ—ç”¨å‡å€¼å¡«å……ï¼ˆæˆ–æ ¹æ®å®é™…æƒ…å†µå¤„ç†ï¼‰
                df[col] = pd.to_numeric(df[col], errors='coerce')
                df[col].fillna(df[col].mean(), inplace=True)
    
    # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡
    X = df["air_temperature"].values.reshape(-1, 1)
    y = df["air_temperature"].values.reshape(-1, 1)

    # å¤„ç†ç¼ºå¤±å€¼
    X = np.nan_to_num(X, nan=np.nanmean(X))
    y = np.nan_to_num(y, nan=np.nanmean(y))

    # å½’ä¸€åŒ–
    scaler_x, scaler_y = MinMaxScaler(), MinMaxScaler()
    X_scaled = scaler_x.fit_transform(X)
    y_scaled = scaler_y.fit_transform(y)

    # åˆ›å»ºåºåˆ—æ•°æ®ï¼ˆæ—¶é—´åºåˆ—å¿…é¡»ä¿æŒé¡ºåºï¼Œä¸æ‰“ä¹±ï¼‰
    X_seq, y_seq = create_sequences(X_scaled, y_scaled, seq_len)
    
    # åˆ’åˆ†è®­ç»ƒæµ‹è¯•é›†ï¼ˆæ—¶é—´åºåˆ—ä¸æ‰“ä¹±ï¼‰
    split_idx = int(len(X_seq) * 0.8)
    X_train, X_test = X_seq[:split_idx], X_seq[split_idx:]
    y_train, y_test = y_seq[:split_idx], y_seq[split_idx:]

    # è®¾å¤‡é…ç½®
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device} | åºåˆ—é•¿åº¦: {seq_len} | ç‰¹å¾æ•°: {X_train.shape[2]}")

    # åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    model = LSTMModel(
        input_size=X_train.shape[2],
        hidden_size=hidden_size,
        num_layers=2
    ).to(device)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)  # æ·»åŠ L2æ­£åˆ™åŒ–

    # è½¬æ¢ä¸ºå¼ é‡
    X_train = torch.tensor(X_train, dtype=torch.float32).to(device)
    y_train = torch.tensor(y_train, dtype=torch.float32).to(device)
    X_test = torch.tensor(X_test, dtype=torch.float32).to(device)
    y_test = torch.tensor(y_test, dtype=torch.float32).to(device)

    # è®­ç»ƒè¿‡ç¨‹
    print("ğŸš€ LSTM å¼€å§‹è®­ç»ƒ")
    train_losses = []
    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()
        
        outputs = model(X_train)
        loss = criterion(outputs, y_train)
        
        loss.backward()
        optimizer.step()
        
        train_losses.append(loss.item())
        
        if (epoch + 1) % 10 == 0:
            print(f"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.6f}")

    # æµ‹è¯•é›†è¯„ä¼°
    model.eval()
    with torch.no_grad():
        y_pred = model(X_test).cpu().numpy()
        # åå½’ä¸€åŒ–
        y_pred = scaler_y.inverse_transform(y_pred)
        y_true = scaler_y.inverse_transform(y_test.cpu().numpy())

    # è®¡ç®—æŒ‡æ ‡
    mse = mean_squared_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    print(f"âœ… LSTM å®Œæˆ | MSE={mse:.6f}, RÂ²={r2:.4f}")

    # å¯è§†åŒ–ç»“æœ
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']  # æ”¯æŒä¸­æ–‡çš„å­—ä½“åˆ—è¡¨
    plt.rcParams['axes.unicode_minus'] = False  
    plt.figure(figsize=(12, 6))
    plt.plot(y_true, label='çœŸå®å€¼', color='blue')
    plt.plot(y_pred, label='é¢„æµ‹å€¼', color='red', linestyle='--')
    plt.title('LSTMé¢„æµ‹ç»“æœå¯¹æ¯”')
    plt.xlabel('æ ·æœ¬ç´¢å¼•')
    plt.ylabel('ç›®æ ‡å€¼')
    plt.legend()
    plt.show()

    return  y_pred, y_true


if __name__ == "__main__":
    # è¯»å–æ•°æ®ï¼ˆæ›¿æ¢ä¸ºä½ çš„æ•°æ®è·¯å¾„ï¼‰
    df = pd.read_csv("./lang_rag/data/environment_data_export_2025-10-16_164127.csv")
    # å¯è°ƒæ•´å‚æ•°ï¼šåºåˆ—é•¿åº¦ã€è®­ç»ƒè½®æ•°ã€éšè—å±‚å¤§å°
    print(train(df))
